{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition - Porto Seguro's Safe Driver Prediction\n",
    "\n",
    "This notebook is more about learning from some top solutions rather than inventing my own solutions, although I will certainly give my own attempt.\n",
    "\n",
    "A few things about this competition and why it appeals to me...\n",
    "\n",
    "This is the biggest Kaggle competition to date. (other than Titanic, which I will publish my 'stupid-but-it-works' 100% web scraping solution to soon.) There are 5169 teams in total. Something amazing is the diversity of the solutions. In the first place solution by Michael Jahrer he admitted to a lack of creativity when it came to feature engineering and not much success with XGBoost. Yet in the second place solution by 三个臭皮匠, they relied quite heavily on feature engineering, and XGBoost played quite a vital role in their solution. The 3rd place solution by utility is noteworthy for jumping 1071 spots. (There is a public leaderboard and private leaderboard. Sometimes people overfit to the data in the public leaderboard and get punished when the private leaderboard comes out and their solution goes way down). Another fun thing - The 1st place solution Jahrer says he never drops features. In the 3rd place solution by utility, the first thing utility discusses in their solution is eliminating features.\n",
    "\n",
    "This is from the insurance industry, an industry I have some knowledge and experience in. One of the reasons I moved from the actuarial career path to the data scientist path was because I felt the insurance industry had a bit of a reputation as some sort of big old immovable force that was in need of some new ways of thinking. An oversimplification of insurance pricing goes like...\n",
    "\n",
    "i. You have a base rate and then we determine your risks based on information you provide us.\n",
    "\n",
    "ii. For each risk, we look up a table, and this table gives some modifier to your base rate.\n",
    "\n",
    "iii. That number is what we charge you for your premium\n",
    "\n",
    "So for example, You are 40 and drive a red sports car, we may give you a times 0.95 modifier for your age, add 10 dollars to your premium because your car is red, and then throw a times 1.5 modifier because you drive a sportscar... So if your base rate was 100, you rate would be (100 x 0.95 + 10 ) x 1.5 = 157.5. Before data science and machine learning became super popular and was applied to every problem imaginable, this was a great way to do it. You're looking at huge datasets, there's a value for each risk category, and you can come up with a reasonable assumption based on these actuarial tables. It's like a pseudo decision tree.\n",
    "\n",
    "There are also more 'one price fits all' kinds of insurance... You may have seen this for health insurance. There are different tiers of plans with clear details on what they cover or don't cover, then you buy the plan and you roll with it. The theory with insurance is that if you have a pool of people paying premiums and most don't get sick, that will be enough to cover those few people in the pool who get sick and need money.\n",
    "\n",
    "These one price fits all sort of plans aim to simplify and standardize insurance so that insurers can cut some costs and offer a lower plan. The drawbacks of this is that in reality, people's health problems aren't so cut and dry. For example, we could have two people. Person A's healthcare costs $200 a year, and Person B's healthcare costs $150 a year. Person A has all their medical needs satisfied by the bronze plan. Person B has a slightly more unique medical condition that is only covered by the silver plan, but the actual cost of healthcare is less than Person A. Person B leaves your insurance company because they don't think they should be paying the silver tier. This example is just one person, but this could be a company with 5000 employees leaving your insurance plan because they want a specific health care coverage.\n",
    "\n",
    "And that's it! This first notebook is just very simple. Subsequent notebooks will kind of go into top solutions by Kagglers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv('train.csv')\n",
    "dftest = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595212, 59)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892816, 58)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 58 features. The train set consistes of 595,212 observations, and the test set had 892,816 features.\n",
    "\n",
    "The target variable is conveniently named target, and is broken down by the counts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    573518\n",
       "1     21694\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong imbalance here, and this is something we have to manage because if we just throw our algorithms at it the models could be biased. Common ways to handle this is to either upsample the small class (the 1s), downsample the larger class the (0s), or applying weights in our models.\n",
    "\n",
    "I think it would be a bit redundant to make my own EDA when some other Kagglers have made really good EDAs, so I will refer you to this very in-depth EDA [here.](https://www.kaggle.com/headsortails/steering-wheel-of-fortune-porto-seguro-eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This competition was evaluated using the normalized gini coefficient. The [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient) is often used in economics as an indicator of inequality and is used in the actuarial/industry for evaluating predictive models.\n",
    "\n",
    "A few things about the normalized gini coefficient...\n",
    "\n",
    "1. The normalization just put the gini coefficient from a scale of 0 to 1.\n",
    "\n",
    "2. We wouldn't want to use a regular accuracy here because of the high degree of imbalance in this dataset. If we just predicted everything as 0, we would already have an accuracy in the high 90s.\n",
    "\n",
    "3. The gini coefficient isn't too hard to understand but I like to walk through each step of the algorithm and see how it is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual = np.array([1, 1, 0, 1, 0])\n",
    "pred = np.array([0.1, 0.2, 0.9, 0, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   0.1  0. ]\n",
      " [ 1.   0.2  1. ]\n",
      " [ 0.   0.9  2. ]\n",
      " [ 1.   0.   3. ]\n",
      " [ 0.   0.1  4. ]]\n",
      "\n",
      "[[ 0.   0.9  2. ]\n",
      " [ 1.   0.2  1. ]\n",
      " [ 1.   0.1  0. ]\n",
      " [ 0.   0.1  4. ]\n",
      " [ 1.   0.   3. ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float))\n",
    "print('')\n",
    "print(all[ np.lexsort((all[:,2], -1*all[:,1])) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first array we just have our actual on the first column, predicted on the second column, and the rank on the third column. In our second array we sort the first column based on the probabilites in the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our denominator is simply the sum of our first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  2.  3.]\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "print(all[:,0].cumsum())\n",
    "print(all[:,0].cumsum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our numerator is the sum of the cumulative sum at each point in our first column. So you can see if we had all our 1s higher up, we would have bigger cumulative sums and a larger numerator, resulting in a normalized Gini coefficient closer to 1.\n",
    "\n",
    "All the other steps here are just simple arithmetic meant to make the number more interpretable and pretty, so we'll move on.\n",
    "\n",
    "The function definitions are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert(len(actual) == len(pred))\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "\n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the data... The features are not very revealing at all. The only labels are ind, reg, car, calc, and then an additional label that is either empty, cat, bin, corresponding to continuous, categorical, and binomial.\n",
    "\n",
    "This is both good and bad. I think at the end of the day, the best predictive model is one that relies on both machines and human knowledge. When the features are anonymized like this model, you take away the opportunity for human intuition for feature engineering. But at the same time, you're eliminating a lot of biases that can arise from human intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these are just a few of the early thoughts. We will look into Michael Jahrer's first place solution next!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
